{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024-06-09 - Attempt for a simple chatbot using Llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Required Libraries (likely not needed in running in poetry shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from langchain import LLMPipeline, LLMChain, Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def load_model(model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def verify_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Set up device\n",
    "device = verify_cuda()\n",
    "\n",
    "# Login to Hugging Face via hugingface_hub (for private models)\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd245d80cb4f9da6f9c342d82371e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the environment variable\n",
    "environment = \"dev\"  # prod, dev\n",
    "\n",
    "# Load models based on the environment\n",
    "if environment == \"dev\":  # Development\n",
    "    # model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Use official 8B model (requires access)\n",
    "    model_name = \"aaditya/Llama3-OpenBioLLM-8B\"  # Or 8B version of the OpenBioLLM model\n",
    "    # model_name = \"distilgpt2\"  # Or alternatively, use a much much smaller model that isn't llama-based\n",
    "elif environment == \"prod\":  # Production\n",
    "    model_name = \"aaditya/Llama3-OpenBioLLM-70B\"  # Use the larger model for production\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Please choose either 'dev' or 'prod'.\")\n",
    "\n",
    "model, tokenizer = load_model(model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from huggingface example\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. who's willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How can i split a 3mg or 4mg waefin pill so i can get a 2.5mg pill?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.0,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave the stuff below for another day (2024/06/10 MK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple template for the prompt\n",
    "template = \"\"\"\n",
    "The following is a conversation between a friendly medical chatbot and a heart-failure patient. The chatbot collects details about symptoms and medication adherence.\n",
    "\n",
    "Patient: {input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"input\"], template=template)\n",
    "\n",
    "# Set up conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a function to generate responses using the model\n",
    "def generate_response(model, tokenizer, device, input_text, memory):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=500)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    memory.add_context(\"Chatbot\", response)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Set up the chatbot with LangChain\n",
    "def run_chatbot(model, tokenizer, device, memory):\n",
    "    print(\"You are chatting with a friendly medical chatbot. Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        prompt_with_memory = memory.get_context() + f\"Patient: {user_input}\\nChatbot:\"\n",
    "        response = generate_response(model, tokenizer, device, prompt_with_memory, memory)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_patient_medication_history(patient_id):\n",
    "    # Placeholder function for looking up patient medication history\n",
    "    return f\"Medication history for patient {patient_id}: [Placeholder data]\"\n",
    "\n",
    "def look_up_recovery_day(patient_id, discharge_date):\n",
    "    # Placeholder function for looking up the recovery day\n",
    "    return f\"Patient {patient_id} is on day X of recovery since discharge on {discharge_date}.\"\n",
    "\n",
    "# Example functions to demonstrate tool usage\n",
    "def run_tools_example():\n",
    "    patient_id = \"12345\"\n",
    "    discharge_date = \"2024-01-01\"\n",
    "    print(look_up_patient_medication_history(patient_id))\n",
    "    print(look_up_recovery_day(patient_id, discharge_date))\n",
    "\n",
    "# Run the tool examples\n",
    "run_tools_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chatbot(model, tokenizer, device, memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
