# Note: This code snippet needs to be copy pasted from notebooks/chatbot.ipynb to reco_analysis/reco_analysis/chatbot.py
from dotenv import load_dotenv
from typing import Optional, List
import uuid
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import HumanMessagePromptTemplate, MessagesPlaceholder
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from reco_analysis.chatbot.session_management import get_session_history
from reco_analysis.chatbot.prompts import system_message_doctor, ai_guidance_doctor, ai_guidance_patient

# Load environment variables
load_dotenv("../.env")

# Define the session store (using a dictionary to store chat histories in memory for now)
session_store = {}

model = ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo')

class DialogueAgent:
    def __init__(self,
                 role: Optional[str] = "Doctor",
                 system_message: Optional[str] = system_message_doctor,
                 model: Optional[ChatOpenAI] = model,
                 session_id: Optional[str] = None) -> None:
        """
        Initialize the DialogueAgent with a name, system message, guidance after each run of the chat,
        a language model, and a session ID.
        
        Args:
            role (str): The role of the agent (either 'Patient' or 'Doctor').
            system_message (str): The initial system message to set the context.
            model (ChatOpenAI): The language model to use for generating responses.
            session_id (str, optional): The unique session ID for the conversation. Defaults to None. If None, a new session ID will be generated. If a session ID is provided, the conversation history will be loaded from the session store (if available)
        """
        self.system_message = system_message
        self.model = model

        # Set the role of the agent and the human
        role = role.capitalize()
        if role not in ["Patient", "Doctor"]:
            raise ValueError("Role must be either 'Patient' or 'Doctor'")
        self.role = role
        self.human_role = "Doctor" if self.role == "Patient" else "Patient"

        # Generate a unique conversation ID if one is not provided
        self.session_id = str(uuid.uuid4()) if session_id is None else session_id

        # Initialize chat message history to keep track of the entire conversation
        self.memory = get_session_history(self.session_id, session_store)
        
        # Define the prompt template with placeholders for the chat history and human input
        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=self.system_message),  # The persistent system prompt
                MessagesPlaceholder(variable_name="chat_history"),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template("{human_input}"),  # Where the human input will be injected
            ]
        )
        
        # Define the LLM chain with the model and prompt
        self.chain = self.prompt | self.model | StrOutputParser()

        # Prepare the AI instruction (this acts as guidance for the agent after each run of the chat)
        self.ai_instruct = ai_guidance_doctor if self.role == "Doctor" else ai_guidance_patient

    def reset(self) -> None:
        """
        Resets the conversation history in memory
        """
        self.memory.clear()

    def generate_response(self) -> str:
        """
        Generates a response based on the conversation history stored in memory.
        
        Returns:
            str: The response generated by the language model.
        """
        # Prepare the input for the model
        input_data = {
            "chat_history": self.memory.messages,
            "human_input": self.ai_instruct
        }

        # Run the chain to generate a response
        response = self.chain.invoke(input_data)

        # Save the AI's response to the memory
        self.send(response)

        return response
    
    def send(self, message: str) -> None:
        """
        Adds a new message to the conversation history in memory for the AI role.

        Args:
            message (str): The content of the message.
        """
        # Save the AI response to the conversation memory
        self.memory.add_message(AIMessage(content=message, name=self.role))

    def receive(self, message: str) -> None:
        """
        Adds a new message to the conversation history in memory for the human role.
        
        Args:
            message (str): The content of the message.
        """
        # Save the user input to the conversation memory
        self.memory.add_message(HumanMessage(content=message, name=self.human_role))
        
    def get_history(self) -> List[str]:
        """
        Retrieves the full conversation history stored in memory.
        
        Returns:
            List[str]: The list of messages in the conversation history.
        """
        formatted_history = []
        for msg in self.memory.messages:
            if isinstance(msg, HumanMessage):
                formatted_history.append(f"{self.human_role}: {msg.content}")
            elif isinstance(msg, AIMessage):
                formatted_history.append(f"{self.role}: {msg.content}")
            else:
                formatted_history.append(f"System: {msg.content}")
        return formatted_history