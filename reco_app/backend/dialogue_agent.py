"""Based on Gary's code in `1.0-gk-milestone1.ipynb`"""

import logging
import os
from typing import Any, Dict, List, Tuple

import dotenv
from langchain import LLMChain
from langchain.memory import ChatMessageHistory
from langchain.prompts import ChatPromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import HumanMessagePromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langsmith import traceable

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


parent_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
dotenv.load_dotenv(f"{parent_dir}/.env", override=True)

# Required environment variables for LangChain: LANGCHAIN_TRACING_V2, LANGCHAIN_ENDPOINT, LANGCHAIN_API_KEY, LANGCHAIN_PROJECT


class UserInterface:
    def __init__(self, agent_role="Doctor"):
        """
        Initialize the User Interface

        Args:
            agent_role (str, optional): The role of the agent. Defaults to 'Doctor'.
        """
        agent_role = agent_role.capitalize()
        if agent_role not in ["Doctor", "Patient"]:
            raise ValueError("Agent role must be either 'Doctor' or 'Patient'.")
        self.agent_role = agent_role  # By default, the agent is a doctor

    def collect_user_input(self):
        """
        Collects user input.

        Returns:
            str: The user input.
        """
        user_input = input("Enter user message. Enter 'exit' to stop chat: ")
        return user_input

    def display_response(self, response: str):
        """
        Displays the response to the user.

        Args:
            response (str): The response to display.
        """
        print(f"{self.agent_role}: {response}")


class DialogueAgent:
    def __init__(
        self,
        role: str,
        system_message: str,
        model: ChatOpenAI = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo"),
        verbose=False,
    ) -> None:
        """
        Initialize the DialogueAgent with a name, system message, and a language model.

        Args:
            role (str): The role of the agent (either 'Patient' or 'Doctor').
            system_message (str): The initial system message to set the context.
            model (ChatOpenAI): The language model to use for generating responses.
            verbose (bool, optional): Whether to print verbose output. Defaults to False.
        """
        self.system_message = system_message
        self.model = model

        # Set the role of the agent and the human
        role = role.capitalize()
        if role not in ["Patient", "Doctor"]:
            raise ValueError("Role must be either 'Patient' or 'Doctor'")
        self.role = role
        self.human_role = "Doctor" if self.role == "Patient" else "Patient"

        # Initialize chat message history to keep track of the entire conversation
        self.memory = ChatMessageHistory()

        # Define the prompt template with placeholders for the chat history and human input
        self.prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=self.system_message
                ),  # The persistent system prompt
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # Where the memory will be stored.
                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # Where the human input will be injected
            ]
        )

        # Define the LLM chain with the model and prompt
        self.chain = LLMChain(
            llm=self.model,
            prompt=self.prompt,
            verbose=verbose,
        )

        # Reset the conversation history
        self.reset()

    def reset(self) -> None:
        """
        Resets the conversation history in memory
        """
        self.memory.clear()

    def generate_response(self) -> str:
        """
        Generates a response based on the conversation history stored in memory.

        Returns:
            str: The response generated by the language model.
        """
        # Retrieve the current conversation history as a list of messages
        conversation_history = self.memory.messages

        # Predict the next response using the chain
        response = self.chain.predict(
            chat_history=conversation_history,
            human_input="",  # No additional input because the AI is generating the response
        )

        # Save the AI's response to the memory
        self.send(response)

        return response

    def send(self, message: str) -> None:
        """
        Adds a new message to the conversation history in memory for the AI role.

        Args:
            message (str): The content of the message.
        """
        # Save the AI response to the conversation memory
        self.memory.add_message(AIMessage(content=message, name=self.role))

    def receive(self, message: str) -> None:
        """
        Adds a new message to the conversation history in memory for the human role.

        Args:
            message (str): The content of the message.
        """
        # Save the user input to the conversation memory
        self.memory.add_message(HumanMessage(content=message, name=self.human_role))

    def get_history(self) -> List[str]:
        """
        Retrieves the full conversation history stored in memory.

        Returns:
            List[str]: The list of messages in the conversation history.
        """
        formatted_history = []
        for msg in self.memory.messages:
            if isinstance(msg, HumanMessage):
                formatted_history.append(f"{self.human_role}: {msg.content}")
            elif isinstance(msg, AIMessage):
                formatted_history.append(f"{self.role}: {msg.content}")
            else:
                formatted_history.append(f"System: {msg.content}")
        return formatted_history
